\documentclass[10pt]{elegantbook}

\title{Machine Learning Notes}
\subtitle{All in the data.}

\author{occupymars}
\date{June. 23, 2025}
\version{0.1}

\cover{iron_man.jpg}

% all the packages included
\usepackage{cprotect}
\usepackage{fontawesome}
\usepackage[linesnumbered, ruled]{algorithm2e}
\RestyleAlgo{algoruled}

% set some fonts
\setmonofont{Ubuntu Mono}

% my own commands
% \newcommand{\mydefination}[1]{\textit{\textcolor[RGB]{0,174,247}{#1}}}
\newcommand{\mydefination}[1]{\textbf{\textit{\textcolor{structurecolor}{#1}}}}

\begin{document}

\maketitle

\frontmatter
\tableofcontents

\mainmatter

\chapter{Gaussian Process}

\section{Gaussian processes for dynamics learning in model predictive control (2025)}

\subsection{Overview of static Gaussian process regression}

GPR was introduce in the statistics community by \textit{Curve Fitting and Optimal Design for Prediction}, and gained
attention after \textit{Bayesian Learning for Neural Networks} proved that they can be regarded as neural networks of infinite
width.

Given two input data $Z = \{ z_1, \cdots, z_N \}, Z^* = \{ z^*_1, \cdots, z^*_N \}$, using the GP prior, we get:
\[
\begin{bmatrix}
    g_Z \\ g_{Z^*}
\end{bmatrix} \sim \mathcal N 
\left (
\begin{bmatrix}
    0 \\ 0
\end{bmatrix},
\begin{bmatrix}
    \mathscr{K}_{Z, Z} & \mathscr{K}_{Z, Z^*} \\
    \mathscr{K}_{Z^*, Z} & \mathscr{K}_{Z^*, Z^*}
\end{bmatrix}    
\right )
\]

Now given observation for $Z$ as $Y = [y_1, \cdots, y_N]$, the posterior can be written as:
\[
p(g_Z, g_{Z^*} \mid Y) = \frac{p(Y \mid g_Z)p(g_Z, g_{Z^*})}{p(Y)}
\]

The kernel is often taken as Gaussian one:
\[
\mathscr{K}_{Z, Z^*} = \lambda \exp \left \{ -\frac{\| Z - Z^* \|^2}{2 \eta} \right \}
\]

If we are using the measurement model $y_i = g(z_i) + w_i$ and assume noise term is independent from prior $g$:
\[
Y \mid g_Z \sim \mathcal N (g_Z, \sigma_w^2 I_N)
\]
so the posterior is also Gaussian, we can get the posterior of interest:
\[
p(g_{Z^*} \mid Y) = \int_{\mathcal Z} \frac{p(Y \mid g_Z)p(g_Z, g_{Z^*})}{p(Y)} dg_Z
\]
then compute the first and second order moments, we get $g_{Z^*} \mid Y \sim \mathcal N (\mu(Z^*), \Sigma(Z^*))$:
\begin{equation} \label{eq:gp_mean_cov}
\left \{
\begin{array}{lll}
    \mu(Z^*) &=& \mathscr{K}_{Z^*, Z} (\mathscr{K}_{Z, Z} + \sigma_w^2 I_N)^{-1} Y \\
    \Sigma(Z^*) &=& \mathscr{K}_{Z^*, Z^*} - \mathscr{K}_{Z^*, Z} (\mathscr{K}_{Z, Z} + \sigma_w^2 I_N)^{-1} \mathscr{K}_{Z, Z^*}
\end{array}    
\right .
\end{equation}

\begin{remark}
    1, Alternative paradigms for uncertainty quantification, from RKHS to multi-arm bandits, frequency methods...
\end{remark}

The hyperparameters are estimated from a subset of data $(Z_h, Y_h)$ by optimizing the marginal likelihood:
\begin{equation} \label{eq:hyper_opt}
\text{argmax}_{\xi} ~ p(Y_h \mid \xi) = \text{argmax}_{\xi} ~ \int_{\mathbb R^N} p(Y_h \mid g_{Z_h}, \xi)p(g_{Z_h} \mid \xi)dg_{Z_h}
\end{equation}
if the measurement is i.i.d., then $Y_h \mid \xi \sim \mathcal N(0, \mathscr{K}_{Z_h, Z_h} + \sigma_w^2)$, so the above optimization
problem can be written as a negative-log-likelihood minimization problem:
\begin{equation} \label{eq:hyper_nll_opt}
\text{argmax}_{\xi} ~ Y_h^{\top}(\mathscr{K}_{Z_h, Z_h} + \sigma_w^2)^{-1}Y_h + \log \det(\mathscr{K}_{Z_h, Z_h} + \sigma_w^2)
\end{equation}
we can use a gradient based method to optimize this one, however this cost is not convex, so its result maybe not global minimum and 
thus unreliable. An alternative way is \mydefination{Markov Chain Monte Carlo} approaches, perform numerical integration on \ref{eq:hyper_opt}.

\subsection{Gaussian processes for dynamical systems}
A first option to describe a dynamical system is the Nonlinear, Auto-Regressive with eXogenous input (\mydefination{NARX}) model:
\[
y_i = g_{NARX}(y_{i-1}, \cdots, y_{\tau_y}, u_{i-1}, \cdots, u_{\tau_u}) + w_i
\]

We can write it as the state-space model:
\begin{equation}
    \left \{ 
    \begin{array}{rll}
        x_{i+1} &=& f(x_i, u_i) + v_i \\
        y_i &=& g(x_i) + w_i
    \end{array}    
    \right .    
\end{equation}
where $f$ and $g$ denotes transition and emission maps, typically $g$ is known (even if it is not, we can augment it into 
transition maps). There are two challenges, learning two maps and state inference (from $y$ get $x$), that is tackled by two
different approaches in academic:
\begin{itemize}
    \item Optimizing latent state variables: treate the state variables as optimization variables, jointly optimize it with 
model parameters to get maximum likelihood.
    \item Alternating function learning and state inference: this method try to extend Bayesian techniques such as 
\mydefination{Extended Kalman Filter}, \mydefination{Unscented Kalman Filter}, Assumed Density Filter, and Particle Filter 
to non-parametric models, the approximation in these 
studys are Taylor expansions, exact moment matching and particle representations. But when the state measurement
are not available, we have to iteratively alternate between inferring the posterior and updating $ \xi $ to maximize the 
marginal likelihood, using algorithm like \mydefination{Expectation Maximization}. The approximation to decrease the computational
complexity are truncated orthogonal basis functions expansions (see [132, 133, 134]) and variational inference.
\end{itemize}

\subsection{Porblem formulation}
The discrete model dynamic:
\begin{equation} \label{eq:gp_dynamic_model}
    x_{i+1} = g_{nom}(x_i, u_i) + B_d g(x_i, u_i) + v_i
\end{equation}
where $g_{nom}: \mathbb R^{n_x \times n_u} \rightarrow \mathbb R^{n_x}, g: \mathbb R^{n_x \times n_u} \rightarrow \mathbb R^{n_d}$, 
if we do not have nominal model, then $B_d = I_{n_x}$.

And we use $z_i = [x_i^{\top} \quad u_i^{\top}]^{\top}$, we will train $n_d$ GPs for each dimension separately.

The optimal control problem is then:
\begin{align}
    \text{minimize}_{\{\pi_i\}} \quad & \mathbb{E} \left[ \bar{\mathcal{L}}_{\bar T}(x_{\bar T}) + \sum_{i=0}^{\bar T-1} \mathcal{\bar L_i}(x_i, u_i) \right] \\
    \text{subject to} \quad & x_{i+1} = g_{\text{nom}}(x_i, u_i) + B_d g(x_i, u_i) + v_i \\
    & u_i = \pi_i(x_i) \\
    & \mathbb{P}(h_j(x_i, u_i) \leq 0, \forall i \geq 0) \geq p_j \quad \forall j=1,\ldots,n_h \\
    & x_0 = \bar{x}_0
\end{align}

This problem is hard to solve, so we transform it to a MPC problem at time step $k$:
\begin{align}
    \text{minimize}_{\{\pi_i | k\}} \quad & \mathbb{E} \left[ \mathcal{L}_{T}(x_{T|k}) + \sum_{i=0}^{T-1} \mathcal{L_i}(x_{i|k}, u_{i|k}) \right] \\
    \text{subject to} \quad & x_{i+1|k} = g_{\text{nom}}(x_{i|k}, u_{i|k}) + B_d g(x_{i|k}, u_{i|k}) + v_{i|k} \\
    & u_{i|k} = \pi_{i|k}(x_{i|k}) \\
    & \mathbb{P}(h_j(x_{i|k}, u_{i|k}) \leq 0, \forall i \geq 0) \geq p_j \quad \forall j=1,\ldots,n_h \\
    & x_{0|k} = x_k
\end{align}

\subsection{Scalable methods for GPR}
More detailed survey please refer to \textit{When Gaussian Process Meets Big Data: A Review of Scalable GPs}.

\begin{table}[ht]
\centering
\caption{Computational Complexity of Gaussian Process Methods.}
\begin{tabular}{lccccccc}
\toprule
& GP Full & Subset of Data & Expert-based & FTC & SSGP & SKI & SVGP \\
\midrule
Training  & $\mathcal O(N^3)$ & $\mathcal O(M^3)$ & $\mathcal O(NM_e^2)$ & $\mathcal O(NM^2)$ & $\mathcal O(Np^2)$ & $\mathcal O(N + M \log M)$ & $\mathcal O(M^3)$ \\
Inference & $\mathcal O(N^2)$ & $\mathcal O(M^2)$ & $\mathcal O(M_e^2)$  & $\mathcal O(M^2)$  & $\mathcal O(p^2)$  & $\mathcal O(M \log M)$     & $\mathcal O(M^2)$ \\
\bottomrule
\end{tabular}
\end{table}

\mydefination{1. Subset of Data}, sample data using some criterion (refer to \textit{Gaussian Process Models: PAC-Bayesian 
Generalisation Error Bounds and Sparse Approximations}, chapter 4) and clustering. This will overestimate uncertainty, but 
new study leveraging graphons complementss rigorous bounds for it. 

We can also use multiple models for different regions for non-Stationarity or scalability. One of them is called \mydefination{Mixture-of-Experts}
(MoE), given $N_{exp}$ GPs, denoting with $\{ s_k(\cdot) \}_{k=1}^{N_{exp}}$ a set of gating functions, the overall likelihood is
\[
p_{MoE}(y \mid g_z^1, \cdots, g_z^{N_{exp}}) = \sum_{k=1}^{N_{exp}} s_k(g_z^k)p_k(y \mid g_z^k)
\]
to scale well, we need to use infinite MoE or one of the approximation methods. We can also pre-allocate experts but this will
lose connection between experts. See [166], [167] for online updates. And there is another method called "bagging".

Instead of resorting to a linear combination of GPs, we can use \mydefination{Product-of-Experts} (PoE), where
\[
p_{PoE}(y \mid g_z^1, \cdots, g_z^{N_{exp}}) \propto \prod_{k=1}^{N_{exp}} p_k(y \mid g_z^k)
\]
this will make weak expert plays which is not good, so we can use weighted product and Bayesian Committee Machine, combined
we have [174]. MoE and PoE combine in \textit{Deep Structured Mixtures of Gaussian Processes}. Analysis of theory 
in \textit{An asymptotic analysis of distributed nonparametric methods}.

\mydefination{2. Inducing Variables}, given inducing points (pseudo-inputs) $\bar Z$ and $g_{\bar Z} \sim \mathcal N(0, \mathscr{K}_{\bar Z, \bar Z})$, 
$g_Z$ and $g_{Z^*}$ are conditionally independent, they can only communicate through $g_{\bar Z}$. There are two main groups,
one is approximate prior $p(g_Z, g_{Z^*})$ and do exact inference (reviewed in \textit{A Unifying View of Sparse Approximate Gaussian Process Regression}),
one is from original prior and approximate $p(g_{Z^*} \mid Y)$ (reviewed in \textit{A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation}),
they are compared in \textit{Understanding Probabilistic Sparse Gaussian Process Approximations}.

First we talk about method approximating prior, with:
\begin{equation}
    \begin{array}{rll}
        p(g_{Z},g_{Z^{*}}) &=& \int p(g_{Z},g_{Z^{*}}|g_{\bar Z})p(g_{\bar Z})d g_{\bar Z} \\ 
        &\approx&\int q(g_{Z}|g_{\bar Z})q(g_{Z^{*}}|g_{\bar Z})p(g_{\bar Z})d g_{\bar Z} \\
        &=& q(g_{Z},g_{Z^{*}})
    \end{array}
\end{equation}
the choice of $q(g_{Z}|g_{\bar Z}) = \mathcal N(\mathscr{K}_{Z, \bar Z}\mathscr{K}_{\bar Z, \bar Z}^{-1}g_{\bar Z}, \tilde Q_{Z, Z})$ 
and $q(g_{Z^{*}}|g_{\bar Z}) = \mathcal N(\mathscr{K}_{Z^*, \bar Z}\mathscr{K}_{\bar Z, \bar Z}^{-1}g_{\bar Z}, \tilde Q_{Z^*, Z^*})$ 
will differ between different methods below. The conditional distribution is actully $q(g_{Z}|g_{\bar Z}) = \mathcal N(\mathscr{K}_{Z,
 \bar Z}\mathscr{K}_{\bar Z, \bar Z}^{-1}g_{\bar Z}, \mathscr{K}_{Z, Z} - \mathscr{K}_{Z, \bar Z}
 \mathscr{K}_{\bar Z, \bar Z}^{-1}\mathscr{K}_{\bar Z, Z}$), 
refer to \href{https://statproofbook.github.io/P/mvn-cond.html}{proof}, $\tilde Q$ is a low rank matrix.
\begin{itemize}
    \item \mydefination{Subset of Regressors} (SoR), $\mathscr{K}_{Z, Z} \approx \mathscr{K}_{Z, \bar Z}\mathscr{K}_{\bar Z, \bar Z}^{-1}\mathscr{K}_{\bar Z, Z} = Q_{Z, Z}$,
so covariance of $q(g_{Z}|g_{\bar Z})$ and $q(g_{Z^{*}}|g_{\bar Z})$ is $\tilde Q_{Z, Z} = Q_{Z, Z} - \mathscr{K}_{Z, \bar Z}\mathscr{K}_{\bar Z, \bar Z}^{-1}\mathscr{K}_{\bar Z, Z}= 0$, 
possibly leading to overconfident predictions. $g_{Z^{*}} = \mathscr{K}_{Z^*, \bar Z} W_{\bar Z}, W_{\bar Z} \sim \mathcal N(0, \mathscr{K}_{\bar Z, \bar Z}^{-1})$, 
$W_{\bar Z}$ can also be written as $\mathscr{K}_{\bar Z, \bar Z}^{-1} \bar Z$.
    \item \mydefination{Deterministic Training Conditional} (DTC), same mean of SoR, covariance is more sensible but the result 
is an inconsistent GP (train with low-rank approximation, test with full variance).
    \item \mydefination{Fully Independent Conditional} (FIC), assume $g_{Z}$ and $g_{Z^{*}}$ are independent of $g_{\bar Z}$,
and \mydefination{Fully Independent Training Conditional} (FITC) admits the factorization on the training conditional only, at the price of having again an inconsistent
GP. \textbf{If the prediction is to be performed on a single point, this two method coincide}.
\begin{equation}
    \begin{array}{l}
        {\mu}^{F I T C}(Z^{*})=\mathscr{K}_{Z^{*}, \bar{Z}},{Q}{}^{-1}\mathscr{K}_{\bar{Z},Z}(\Lambda+\sigma_{w}^{2}I_{N})^{-1}Y \\ 
        \Sigma^{F I T C}(Z^{*})=\mathscr{K}_{Z^{*}, Z^{*}}-\mathscr{K}_{Z^{*}, \bar{Z}}(\mathscr{K}^{-1}_{\bar{{{Z}}},\bar{{{Z}}}}-Q^{-1})\mathscr{K}_{\bar{{{Z}}},Z^{*}}+\sigma_{w}^{2}
    \end{array}
\end{equation}
where $Q = \mathscr{K}_{\bar{Z}, \bar Z} + \mathscr{K}_{\bar{Z},Z}(\Lambda+\sigma_{w}^{2}I_{N})^{-1}\mathscr{K}_{Z, \bar Z}$ and 
$\Lambda_{a, a} = \mathscr{K}_{Z_a, Z_a} - \mathscr{K}_{Z_a, \bar Z} \mathscr{K}_{\bar{Z}, \bar Z}^{-1} \mathscr{K}_{\bar Z, Z_a}, \text{for} ~ a = 1, \ldots, N$.
    \item \mydefination{Partially Independent (Training) Conditional} (PI(T)C) generalizes FI(T)C by
introducing a block structure in the covariance. There maybe no significant improve with respect to FI(T)C.
$\mathscr{K}_{PITC} = Q_{Z, Z} - \text{block.diag}(\mathscr{K}_{Z, Z} - Q_{Z, Z})$.
\end{itemize}

These methods lead to an approximation marginal likelihood:
\begin{equation} \label{eq:inducing_ml}
    q(Y) = \mathcal N(0, \tilde Q_{Z, Z} + \mathscr{K}_{Z, \bar Z}\mathscr{K}_{\bar Z, \bar Z}^{-1}\mathscr{K}_{\bar Z, Z} + \sigma^2_w I_N)
\end{equation}
the choice of inducing points can be same as subset of data method, using information gain, online learning and greedy posterior maximization.
Or we can treat them as hyperparameters, and maximized by \ref{eq:inducing_ml}, which is complicated and may lead to local optimal and over-fitting.
Other methods can be seen in MCMC schemes, which will taker longer traning times.

Second we talk about approximate the posterior. We can use so-called \mydefination{Variational Free Energy} (VFE) to get:
\begin{equation}
    \begin{array}{rll}
    p(g_{Z^*} | Y) &=& \int \int p(g_{Z^*} | g_{Z}, g_{\bar Z})p(g_{Z} | g_{\bar Z}, Y)p(g_{\bar Z} | Y)dg_{Z}g_{\bar Z} \\
    & \approx & q(g_{Z^*}) \\
    &=& \int \int p(g_{Z^*} | g_{\bar Z}) p(g_{Z} | g_{\bar Z}) p(g_{\bar Z} | Y)dg_{Z}g_{\bar Z} \\
    &=& \int p(g_{Z^*} | g_{\bar Z}) p(g_{\bar Z} | Y) dg_{\bar Z} \\
    &\approx & \int p(g_{Z^*} | g_{\bar Z}) q(g_{\bar Z}) dg_{\bar Z}
    \end{array}
\end{equation}
where $p(g_{Z^*} | g_{Z}, g_{\bar Z}, Y) = p(g_{Z^*} | g_{Z}, g_{\bar Z})$ because $Y$ is just a noisy version of $g_Z$ and 
$g_{\bar Z}$ is sufficient.

We then use variational inference to choose $g_{\bar Z}$ and $\bar Z$, by minimizing Kullback-Leibler (KL) divergence:
\[
    \mathcal{KL} \left ( q(g_{Z^{*}},g_{Z}) \| p(g_{Z^{*}},g_{Z}|Y) \right ) = \log p(Y) - \mathbb E_{q(g_{\bar Z},g_{Z})}
\left [ \frac{p(Y, g_{\bar Z},g_{Z})}{q(g_{\bar Z},g_{Z})} \right ]
\]
I think here log is for both term, and I think left side $Z^*$ should be $\bar Z$, or the $=$ should be $\approx$.

In [204] we get $g(g_{\bar Z}) = \mathcal N(\mu_q, \Sigma_q)$, where:
\[
\begin{array}{lll}
    \mu_{q} &=& \sigma_{w}^{-2}\mathscr{K}_{\bar{Z}, \bar{Z}}(\mathscr{K}_{\bar{Z}, \bar{Z}}+\sigma_{w}^{-2}\mathscr{K}_{\bar{Z}, Z}\mathscr{K}_{Z, \bar{Z}})^{-1}\mathscr{K}_{\bar{Z}, Z}Y \\
    \Sigma_q &=& \mathscr{K}_{\bar{Z}, \bar{Z}} (\mathscr{K}_{\bar{Z}, \bar{Z}} + \sigma_{w}^{-2}\mathscr{K}_{\bar{Z}, Z}\mathscr{K}_{Z, \bar{Z}})^{-1} \mathscr{K}_{\bar{Z}, \bar{Z}}
\end{array}
\]
and the hyperparameters can be found by optimizing:
\[
\log p(Y) - \log [ \mathcal N(0,\sigma_{w}^{2}+\mathscr{K}_{Z, \bar{Z}} \mathscr{K}_{\bar{Z}, \bar{Z}}^{-1} \mathscr{K}_{\bar{Z}, Z} ] + \frac{1}{\sigma^2_w} 
\text{Tr}(\mathscr{K}_{Z, Z} - \mathscr{K}_{Z, \bar{Z}} \mathscr{K}_{\bar{Z}, \bar{Z}}^{-1} \mathscr{K}_{\bar{Z}, Z})
\]
the predictive distribution is the same as the one obtained in the DTC approach, but the optimization problem yielding 
hyperparameters and pseudo-inputs differs by the last  addendum, which plays the role of a regularizer and
acts against over-fitting. And the relation with FITC in [220], [221] for mini-batch training. Pseudo-inputs can be set 
arbitraily, or use \mydefination{SKI} (structured kernel interpolation, kernel with structure good for efficient calculation), 
or from training inputs.

Sparse GP error estimation in [230], [231].

\mydefination{3. Finite-dimensional representations} of the kernel operator is next method, as above sparse GP revolve
around the concept of eigen-decomposition of Gram matrices $\mathscr{K}_{Z, Z}$, this one consider eigen-decomposition of the kernel operator
$\mathscr{K}: Z \times Z \rightarrow \mathbb R$. 

The first one is \mydefination{Sparse Spectrum Gaussian processes} (SSGP), introduced in [234], using sum of Fourier features. 
It is suitable for stationary kernels:
\[
\mathscr{K}(z_a, z_b) = \int \exp \{ 2 \pi i s^{\top} (z_a - z_b) \} S(s)ds
\]
where $S(s)$ is proportional to $p_S(s)$, for Gaussian kernel:
\[
p_{S}(s)= \frac{1}{\lambda} \int \exp\{-2\pi i s^{\mathsf{T}}(z_{a}-z_{b})\}{\mathscr{K}}(z_{a},z_{b})d z = \sqrt{2\pi\eta^{n_{z}}}\exp\{-2\pi^{2}\eta \| s \|^{2}\}
\]
which is a multivariate Gaussian. Using MC method, sample $\{ s_r, -s_r \}_{r=1}^{M/2}$ to respect symmetry and exploiting 
trigonometric identities:
\[
\mathscr{K}(z_{a},z_{b})\approx\frac{\lambda}{M}\sum_{r=1}^{M/2} \left (\cos(2\pi s_{r}^{\top}z_{a})\cos(2\pi 
s_{r}^{\top}z_{b}) + \sin(2\pi s_r^{\top} z_a) \sin(2\pi s_r^{\top} z_b) \right )
\]
Then the kernel is approximate to $\mathscr{K}(z_{a},z_{b}) = \left\langle\phi(z_{a}),\phi(z_{b})\right\rangle$, where 
\[
\phi(z) = [\cos(2\pi s_{1}^{\textsf{T}}z),\cdots,\cos(2\pi s_{M/2}^{\textsf{T}}z),\cdots ,\sin(2\pi s_{M/2}^{\textsf{T}}z)]^{\top}
\]
then $g(z) \approx \phi(z)^{\top} \alpha$.

Another one aims for a series expansion of the type (see Mercer's Theorem):
\[
\mathscr{K}(z_{a},z_{b}) = \sum_{k=1}^{+\infty}\gamma_{k}\varphi_{k}(z_{a})\varphi_{k}(z_{b})
\]
such that $\int \mathscr{K}(z_{a},z_{b})\varphi_{k}(z_{a})p_{z}(z_{a})d z_{a}=\gamma_{k}\varphi_{k}(z_{b})$, where 
$\{ \varphi_k \}_{k=1}^{+\infty}$ is a family of orthonormal functions with respect to the measure induced by $p_z(\cdot)$,
and $\{ \gamma_k \}_{k=1}^{+\infty}$ is a set of decreasing, non-negative values.

\mydefination{4. Computational techniques} is the last topic in this section. 

First we talk about \mydefination{Nystrom approximation}, $\mathscr{K}_{Z, Z} \approx \mathscr{K}_{Z, 
\bar Z}\mathscr{K}_{\bar Z, \bar Z}^{-1}\mathscr{K}_{\bar Z, Z}$, can be efficiently obtained from an incomplete (block)
column-based Cholesky decomposition when the same (block) columns are selected.

Secondly, \mydefination{Conjugate gradient} (CG) method is for optimization problem $\min_x \frac{1}{2} x^{\top} A x -x^{\top}b$,
iteratively refines its solution estimate by performing the minimization successively in a growing subspace of orthogonal 
search directions. Computational complexity is $\mathcal O(PN^2)$, where $P$ is the iteration, and CG can guarantee to when 
$P = N$, it will recover the exact solution. But we will make $P \ll N$ to get efficience while get a good approximation.
With efficient MVMs (matrix-vector multiplications) of SKI, CG can speed up sparse GP.

Thirdly, the \mydefination{parallelization of computations}, for MoE, inducing point method (PITC has the block structure, using low-rank-cum-Markov
approximations), Nystrom approximation (row-based Cholesky decomposition), CG (batch version).

\mydefination{5. online learning}:
\begin{itemize}
    \item \mydefination{Active-data selection}, using a variety of methods to add or delete data points in dataset.
    \item Expert-based methods, compute the distance of new data and each local model, and update corresponding local model.
    \item Incuding point methods, [358] for FITC, [366] for VFE, a lot of papers mentioned.
    \item Finite-dimensional approximations of kernel operator,  [245] for SSGP and other.
    \item Computational methods, leverage SKI [382]
\end{itemize}

\subsection{Uncertainty Propagation}

Directly predict n-step, [387-389]. Indirect method, iteratively do one-step prediction, neglect non-consecutive states relation, 
the uncertainty prediction can be deteriorate [391].

\mydefination{1. Independent one-step-ahead predictions}, with ramdom $Z^*$:
\begin{equation} \label{eq:posterior_of_ramdom_input}
    p(g_{Z^{*}}|Y)=\int p(g_{Z^{*}}|Z^{*},Y)p(Z^{*})d Z^{*}
\end{equation}
the integral is the product of two Gaussian (if we assume $p(Z^{*}) = \mathcal N(\mu^*, \Sigma^*)$), it is not analytic, 
note that $p(g_{Z^{*}}|Z^{*},Y) = \mathcal N(\mu(Z^*), \Sigma(Z^*))$.
We actually only need first and second moments, which are given as:
\begin{equation} \label{eq:expect_mean_and_cov}
\begin{array}{lll}
\mathbb{E}_{g_{Z^*}}[g_{Z^*} \mid \mathbf{Y}] 
&=& \mathbb{E}_{Z^*}[\mathbb{E}_{g_{Z^*}}[g_{Z^*} \mid Z^*, \mathbf{Y}]] \\
&\overset{\ref{eq:gp_mean_cov}}{=}& \mathbb{E}_{Z^*}[\mu(Z^*)], \\
\text{Var}_{g_{Z^*}}[g_{Z^*} \mid \mathbf{Y}] 
&=& \mathbb{E}_{Z^*}[\text{Var}_{g_{Z^*}}[g_{Z^*} \mid Z^*, \mathbf{Y}]] 
+ \text{Var}_{Z^*}[\mathbb{E}_{g_{Z^*}}[g_{Z^*} \mid Z^*, \mathbf{Y}]] \\
&\overset{\ref{eq:gp_mean_cov}}{=}& \mathbb{E}_{Z^*}[\Sigma(Z^*)] 
+ \text{Var}_{Z^*}[\mu(Z^*)].
\end{array}
\end{equation}

We have three ways to approximate this. First is \mydefination{Linearlization}, using Taylor expansion:
\begin{equation}
    \begin{array}{lll}
        \mu(z^{\ast}) & \approx & \mu(\mu^{\ast})+\nabla_{z}\mu(z)|_{z=\mu^{\ast}}^{\top}(z^{\ast}-\mu^{\ast})
        \\ \Sigma(z^{\ast}) & \approx & \Sigma(\mu^{\ast})+\nabla_{z}\Sigma(z)|_{z=\mu^{\ast}}^{\top}(z^{\ast}-\mu^{\ast}) + 
        \frac{1}{2}(z^{\ast}-\mu^{\ast})^{\top}\nabla_{z}^{2}\Sigma(z)|_{z=\mu^{\ast}}^{\ast}(z^{\ast}-\mu^{\ast})
    \end{array}
\end{equation}
which then:
\begin{equation}
\begin{array}{lll}
\mathbb{E}_{Z^*}[\mu(Z^*)] &\approx& \mathbb{E}_{Z^*}[\mu(\mu^*) + \nabla_z \mu(z)\big|_{z = \mu^*}^{\top} (Z^* - \mu^*)] 
= \mu(\mu^*), \\
\mathbb{E}_{Z^*}[\Sigma(Z^*)] &\approx& \mathbb{E}_{Z^*}[\Sigma(\mu^*) + \nabla_z \Sigma(z)\big|_{z = \mu^*} (Z^* - \mu^*) + \frac{1}{2} (Z^* - \mu^*)^\top \nabla_z^2 \Sigma(z)\big|_{z = \mu^*} (Z^* - \mu^*)] \\
&=& \Sigma(\mu^*) + \frac{1}{2} \mathrm{Tr}\left\{\nabla_z^2 \Sigma(z)\big|_{z = \mu^*} \Sigma^*\right\}, \\
\mathrm{Var}_{Z^*}[\mu(Z^*)] &\approx& \mathrm{Var}_{Z^*}[\mu(\mu^*) + \nabla_z \mu(z)\big|_{z = \mu^*}^{\top} (Z^* - \mu^*)] 
= \nabla_z \mu(z)\big|_{z = \mu^*}^{\top} \Sigma^* \nabla_z \mu(z)\big|_{z = \mu^*}.
\end{array}
\end{equation}

Secondly, we have \mydefination{Exact Moment Matching}, if we using Gaussian kernel and assume Gaussian input, then the 
integral of \ref{eq:expect_mean_and_cov} can be derived:
\[
\mathbb{E}_{z^{*}}[\mu(z^{*})]=\int\mu(z^{*})p(z^{*})d z^{*} = \beta^{\top} \mathbf l
\]
where $\beta = (\mathscr{K}_{Z, Z} + \sigma_w^2 I_N)^{-1} Y$ and $\mathbf l = [l_1, \ldots, l_N]$ is function of $z_i, \mu^*
, \Sigma^*$ and hyperparameters.
\[
\begin{array}{lll}
    \text{Var}_{g_{Z^*}}[g_{Z^*} \mid \mathbf{Y}] 
    &=& \mathbb{E}_{Z^*}[\Sigma(Z^*)] + \mathbb{E}_{Z^*}[\mu^2(Z^*)] - \left( \mathbb{E}_{Z^*}[\mu(Z^*)] \right)^2 \\
    &=& \beta^\top L \beta - \mathrm{Tr} \left\{ \left( \mathscr{K}_{Z,Z} + \sigma_w^2 I_N \right)^{-1} L \right\} - \left( \beta^\top \mathbf{l} \right)^2,
\end{array}
\]

Thirdly, we have \mydefination{Sigma-point propagation}, we compute sigma-point for $z^* \sim \mathcal (\mu^*, \Sigma^*)$,
we denote by $\{\bar{z}_{j}^{*}\}_{j=0}^{2n_{z}}$, where $n_z$ is the dimensionality of $z$.
\begin{equation}
    \begin{array}{lll}
        \bar{z}_{0}^{*} &=& \mu^{*} \\
        \bar{z}_{j}^{*} &=& \mu^{*}+\sqrt{n_{z}+\lambda_{m m}}[\mathrm{chol}(\Sigma^{*})]_{j}, \quad \text{for} ~j=1, \ldots, n_z \\
        \bar{z}_{j}^{*} &=& \mu^{*}-\sqrt{n_{z}+\lambda_{m m}}[\mathrm{chol}(\Sigma^{*})]_{j}, \quad \text{for} ~j=n_z + 1, \ldots, 2n_z \\
    \end{array}
\end{equation}
where $[\mathrm{chol}(\Sigma^{*})]_{j}$ is the $j-th$ column of the Cholesky factorization of matrix $\Sigma^*$, and $\lambda_{m m}$
is a user-defined parameter representing how far the sigmapoints are spread from the mean. By viewing the $g$ as $g = \mu + \tilde w$,
where $\tilde w \sim \mathcal N(0, \Sigma)$ is treated as "process noise", we can approximate $\mathbb E_{g_{Z^*}}[g_{Z^*} | Y]$ by
evaluating $\mu$ on the sigma-points:
\[
\mathbb{E}_{g_{z^{*}}}[g_{z^{*}} | Y]\approx\mu_{\mathrm{sp}}=\sum_{j=0}^{2n_{z}}W_{j}^{m}\mu({\bar{z}_{j}^{*}}).
\]
and the variance is:
\[
\text{Var}_{g_{z^{*}}}[g_{z^{*}} | Y] \approx\sum_{j=0}^{2n_{z}}W_{j}^{v}(\mu(\bar{z}_{j}^{*})-\mu_{\mathrm{sp}})(\mu(\bar{z}_{j}^{*})-\mu_{\mathrm{sp}})^{T}+\Sigma(\mu^{*}).
\]
the weights are choose to sum to 1, how to tune them can be found in [117]. This method can be compared to MC method, first 
one choose the points deterministicly, second one randomly (but it does not need the assumption of GP prior).

Fourth is the numerical approximation, \ref{eq:posterior_of_ramdom_input} can be computed using MC. [403] tackles the computational
issue, where the sampling process is guided by a measure of correlation to the previous evaluations.

The above methods handle uncertainty in inference, does not exploit them in training. So here we talk about them, and they 
are connected to treate latent variables as optimization variables in the second section [108]. Or we can use Gaussian mixture 
models [397].

\mydefination{2. Robust uncertainty propagation}, try to give a deterministic bound rather than stochastic one for the propagation.
Model the $z \in \mathcal E(z_p, Q_p)$ which is an ellipsoidal confidence region, and it can maintain its shape throgh the $\tilde{g} = 
g_{\mathrm{nom}}(\bar{z})+\nabla_{z}g_{\mathrm{nom}}(z)|_{z=\bar{z}}^{\mathsf{T}}(z-\bar{z}) + \mu(\bar z)$. This method maybe too 
conservative. More detail please refer to [407].

\mydefination{3. Overcomming the independece assumption}, iteratively recondition the GP model, enabling a
scenario-based [411] control design. while this may face the curse of dimensionality, other method try to approximate it 
with finite horizon [415], [391].

\subsection{Closed-loop Safty Guarantees} \label{Safty_Guarantees}
This section we discuss how to make sure $\mathbb{P}(h_{j}(x_{i},u_{i})\leq0, ~ \forall i\geq0, ~ \forall\{j\}_{j=1}^{n_{h}}) \geq p$, which is a 
joint-in-time chance constraints, every time step must satisfy the constraints.
\begin{table}[h!]
\centering
\caption{Comparison of Closed-Loop Safety Guarantee Methods}
\renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    Method                    & Joint-in-Time          & Conservativeness & Complexity \\
    \midrule         
    6.1 Bounded-Support       & X                      & Low–Medium       & Low \\
    6.2 Robust-in-Probability & Y                      & Medium–High      & Medium–High \\
    6.3 Sampling-Based        & Y                      & Low              & High \\
    6.4 Other Methods         & X (typically)          & Varies           & Low–Medium \\
    \bottomrule
    \end{tabular}
\end{table}
\begin{itemize}
    \item Bounded support assumption: \mydefination{compact set} is bounded and the bound is inside it (for Euler space, other 
    general case need to use Open coverage to define). In this assumption, the error is assume to be bounded with a compact set
    $C_{\delta}=\{\varepsilon:\mathbb{P}(\varepsilon\in C_{\delta})\geq{\mathbf{1}}-\delta\}$, where $\delta$ is a small 
    probability, we can use $\chi^2$ to get the compact set. The error can (is) be $|g - \hat g| = |g - \mu|$, where $\hat g$ gets from the data, the model we trained. 
    This method will make the joint-in-time constraints becomes point-wise-in-time chance constraints so 
    it is not suitable for multi-step prediction.

    \item Robust-in-probability, construct a high confidence interval with a probability of $p$ such that the probability of the model error
    falling within this set ${\cal G}(x,u)$ is $p$, and then perform robust analysis on all possible errors in ${\cal G}(x,u)$.
    Define $\triangle=\left\{g(x,u)\in{\cal G}(x,u) \mathrm{~ for ~ all ~} (x,u)\in\mathcal{Z}\right\}$,
    this method guarantee that $\mathbb P(\triangle) \geq p$. Ensuring that 
    \[ \mathbb{P}(h_{j}(x_{i},u_{i})\leq0, ~ \forall i\geq0, ~ \forall\{j\}_{j=1}^{n_{h}}\mid\triangle)=1 \]
    we can then make sure:
    \[
    \begin{array}{l}
        \mathbb{P}(h_{j}(x_{i},u_{i})\leq0, ~ \forall i\geq0, ~ \forall\{j\}_{j=1}^{n_{h}}) \\
        \geq \mathbb{P}(h_{j}(x_{i},u_{i})\leq0, ~ \forall i\geq0, ~ \forall\{j\}_{j=1}^{n_{h}}\mid\triangle) P(\triangle) \\
        \geq p
    \end{array}
    \]
    a complete survey can be found in [64].
    \textbf{It tries to get the error bound} $|g - \hat g|$, in order to guarantee safty, we have to make sure such a bound exist (and maybe small enough). 
    One of the possible construction can be provided by RKHS method,
$|g - \hat g| \leq \beta_n \sigma_n(x)$, where $\beta_n$ is RKHS norm defined by $|g|_{\mathcal H_k}$, $\sigma_n(x)$ is the variance.
    \item Sampling-based approach, [413], [414], [427] samples GP to get tightened constraints. This method is not conservative,
but it is computational heavy.
    \item other results, generally consider point-wise-in-time chance constraints, can be seen in [432]...
\end{itemize}

\subsection{Discussion}
\mydefination{1. MPC with Scalable GP}, compare of FITC and VFE can be seen in \textit{Understanding Probabilistic Sparse Gaussian Process Approximations},
\textit{A Framework for Evaluating Approximation Methods for Gaussian Process Regression} compare FITC with SoD, MoE and CG. 

And for online learning, [407]
based on RKHSs type bound, feedback linearlization in [437], [438]. \textit{Online learning-based Model Predictive Control with Gaussian Process Models and Stability Guarantees}
recursively update Cholesky factor and give a stability prove.

\mydefination{2. Real-time GPMPC}, the formulation is:
\begin{equation}
    \begin{array}{rl}
        \text{minimize}_{\{u_{i|k},\mu_{i|k}^{x},\Sigma_{i|k}^{x}\}} \quad & \ell_T(\mu_{T|k}^x, \Sigma_{T|k}^x) + \sum_{i=0}^{T-1} \ell_i(\mu_{i|k}^x, u_{i|k}, \Sigma_{i|k}^x) \\
        \text{subject to} \quad & \mu_{i+1|k}^x = \rho(u_{i|k}, \mu_{i|k}^x, \Sigma_{i|k}^x), \quad i = 0, \ldots, T-1, \\
        \quad & \Sigma_{i+1|k}^x = \Phi(u_{i|k}, \mu_{i|k}^x, \Sigma_{i|k}^x), \quad i = 0, \ldots, T-1, \\
        \quad & h_j(\mu_{i|k}^x, u_{i|k}) + \nu_j(u_{i|k}, \mu_{i|k}^x, \Sigma_{i|k}^x) \leq 0, \quad i = 0, \ldots, T-1, \quad j = 1, \ldots, n_h, \\
        \quad & \mu_{0|k}^x = x_k, \\
        \quad & \Sigma_{0|k}^x = 0_{n_x \times n_x}.
    \end{array}
\end{equation}
the problem is the optimization variables are too many, the solution can be
condense solution in [444] (Unmanned Quadrotor), non-condense one in [445, 446]. But still the computations are very heavy.

Another problem is the gradient of the mean and variance, one solution is to neglect the variance, compute it solely on the mean dynamic [448, 442, 449, 450].
Or we can use the last time step solution for the variance, or the last sovler iteration for the quick update.

\mydefination{3. Alternative models for dynamic}, Nonlinear Output Error (NOE) structures, consider the simulation error rather than one step ahead prediction 
error in NARX, so it is a direct method that predict multiple steps. Or use latent variables optimization, leverage the approximate training and variational
inference to get variance into training stage.

We also have so-called \mydefination{Saptio Temporal GP}, consider $g(z_i ; t_k)$, first attempts mainly in geostatistics community. Another way is reformulating
the STGP as Kalman filtering problem, which is reviewed in \textit{The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running}.

Then we can also consider multi-output GP in geostatistics and multi-task learning, by linear combination of latent functions, or by vector-valued GP and multi-output 
RKHS.

\mydefination{4. Alternative uncertainty quantification}, try heuristics $\beta$ in \ref{Safty_Guarantees} second category but no theoretical guarantees. Or 
directly use the GP prior [505], use student-t, kernel embedding. In noise free setting we can use distributional kernel embeddings and sample method, for 
bounded noise we can use multi-step predictors, for additive unbounded noise in linear system (tightening) and nonlinear (conformal prediction). 

\end{document}